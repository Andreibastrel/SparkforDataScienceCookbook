
import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.{SparkContext, SparkConf}
import org.apache.spark.streaming.StreamingContext._
import org.apache.spark.streaming.kafka._
import org.apache.spark.sql._
import org.apache.spark.sql.SQLContext

object DataFrames_Streaming {

  case class Words(wordName:String, count:Int)

  def main(args:Array[String])
  {
    val conf = new SparkConf
    conf.setAppName("StreamingSQL").setMaster ("spark://master:7077")
    val sc = new SparkContext(conf)
    val ssc = new StreamingContext(sc,Seconds(4))
    val kafkaParams = Map("test-consumer-group" -> 1)
    val topicMap = Map("test" -> 1)
    val kafkaLines =
      KafkaUtils.createStream(ssc,"blrovh:2181",
        "test-consumer-group",topicMap)
    val words = kafkaLines.map{tuple => tuple._2}
    val wordPairs = words.map(word => (word,1))
    val reduceWords = wordPairs.reduceByKey((a,b) => a+b)
    reduceWords.foreachRDD{
      rdd =>
      {
        val sqlContext = new SQLContext(rdd.sparkContext)
        import sqlContext.implicits._
        val df = rdd.map(record => Words(record._1, record._2))
        val dfNew = sqlContext.createDataFrame(df)
        dfNew.registerTempTable("Words")
        val data = sqlContext.sql("select wordName from Words")
        data.foreach(row => println(row.toString))
      }
    }

    ssc.start
    ssc.awaitTermination
  } }

