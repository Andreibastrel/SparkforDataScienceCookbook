ipython console -profile=pyspark
In [1]: from pyspark import SparkConf, SparkContext, SQLContext
In [2]: import pandas as pd
In [3]: sqlcontext = SQLContext(sc)
In [4]: pdf1 = pd.DataFrame({'A':['A0','A1','A2','A3'], 'B':
['B0','B1','B2','B3'], 'C':['C0','C1','C2','C3'],'D':
['D0','D1','D2','D3']},index=[0,1,2,3])
In [5]: pdf2 = pd.DataFrame({'A':['A4','A5','A6','A7'], 'B':
['B4','B5','B6','B7'], 'C':['C4','C5','C6','C7'],'D':
['D4','D5','D6','D7']},index=[4,5,6,7])
In [6]: frames = [pdf1,pdf2]
In [7]: result = pd.concat(frames)
In [8]: df1 = sqlcontext.createDataFrame(pdf1)
In [9]: df2 = sqlcontext.createDataFrame(pdf2)
In [10]: df1.unionAll(df2).show()
+---+---+---+---+
| A | B | C | D |
+---+---+---+---+
| A0| B0| C0| D0|
| A1| B1| C1| D1|
| A2| B2| C2| D2|
| A3| B3| C3| D3|
| A4| B4| C4| D4|
|A5 | B5| C5| D5|
| A6| B6| C6| D6|
| A7| B7| C7| D7|
+---+---+---+---+

In [11]: result
Out[11]:
  A  B  C  D
0 A0 B0 C0 D0
1 A1 B1 C1 D1
2 A2 B2 C2 D2
3 A3 B3 C3 D3
4 A4 B4 C4 D4
5 A5 B5 C5 D5
6 A6 B6 C6 D6
7 A7 B7 C7 D7

In [12]: left = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3'],
'A': ['A0', 'A1', 'A2', 'A3'],
'B': ['B0', 'B1', 'B2', 'B3']})
In [13]: right = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3'],
'C': ['C0', 'C1', 'C2', 'C3'],
'D': ['D0', 'D1', 'D2', 'D3']})
In [14]: result = pd.merge(left, right, on='key')
In [15]: result
Out[25]:
  A  B key C  D
0 A0 B0 K0 C0 D0 
1 A1 B1 K1 C1 D1
2 A2 B2 K2 C2 D2
3 A3 B3 K3 C3 D3

In [26]: left_df = sqlContext.createDataFrame(left)
In [27]: right_df = sqlContext.createDataFrame(right)
In [28]: result_df = left_df1.join(right_df, left_df1.key ==
right_df.key, "leftOuter")
In [29]: result_df.show()
+---+---+---+---+---+---+
| A | B |key| C | D |key|
+---+---+---+---+---+---+
| A0| B0| K0| C0| D0| K0|
| A1| B1| K1| C1| D1| K1|
| A2| B2| K2| C2| D2| K2|
| A3| B3| K3| C3| D3| K3|
+---+---+---+---+---+---+
